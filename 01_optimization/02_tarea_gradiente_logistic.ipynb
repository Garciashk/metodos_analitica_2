{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8aCaXIGlnuJ"
      },
      "source": [
        "# Implementación de Gradiente Descendente para Regresión Logística\n",
        "\n",
        "En el curso aprendimos que utilizando el Método del Gradiente Descendente, es posible encontrar aquellos coeficientes que minimizan una función de pérdida. En la clase se demostró el uso de este método para obtener los coeficientes de una Regresión Lineal.\n",
        "\n",
        "En esta tarea se buscará obtener los coeficientes en una regresión logística, la cual no puede resolverse de manera analítica. \n",
        "\n",
        "\n",
        "Para comenzar, generamos datos simulados utilizando [Sci-Kit Learn Datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html). La visualización de estos datos se presenta más abajo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
        "\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/juancop/metodos_analitica_2/blob/main/01_optimization/02_tarea_gradiente_logistic.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/juancop/metodos_analitica_2/blob/dev/01_optimization/02_tarea_gradiente_logistic.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /></a>\n",
        "  </td>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpY4ZLVvmb2K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFwnBvWUofoQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=100, centers=2, n_features=2,\n",
        "                  random_state=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snVhiVtIomrY"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 5))\n",
        "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1])\n",
        "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqw7c_-72KXj"
      },
      "outputs": [],
      "source": [
        "y = y.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXUgERKu05J8"
      },
      "outputs": [],
      "source": [
        "X = np.hstack([np.ones((X.shape[0], 1)), X]) # Añadimos Intercepto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA21fTb8mg-D"
      },
      "source": [
        "En clase se vio que la función a maximizar es la Función de Verosimilitud, que corresponde a:\n",
        "\n",
        "$$\\ell(\\beta) = \\sum_{i = 1}^N \\Big[ y_i ln(\\Lambda(x_i'\\beta)) + (1 - y_i) ln(1 - \\Lambda(x_i'\\beta)) \\Big].$$\n",
        "\n",
        "Teniendo en cuenta que, \n",
        "\n",
        "$$\\frac{\\delta \\ell(\\beta)}{\\delta \\beta_j } = \\sum_{i = 1}^N (y_i - \\Lambda(x_i'\\beta))x_j, $$\n",
        "\n",
        "genere el código que permita implementar la regla de actualización del gradiente para maximizar la verosimilitud. \n",
        "\n",
        "**Nota:** Recuerde que $-\\ell(\\beta)$ es conocida como la *Binary Crossentropy* y el resultado de su minimización coincide con la maximización de la Verosimilitud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw7JFy0huJR1"
      },
      "source": [
        "# Paso 1. Construcción Función de Activación\n",
        "\n",
        "Para poder realizar la optimización es necesario tener a la mano todos los elementos que la componen. Este caso necesitamos programar la función que nos genera la probabilidad -- la función sigmoide. \n",
        "\n",
        "Esta función viene dada por, \n",
        "\n",
        "$$\\Lambda(z) = \\frac{1}{1 + exp(-z)}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmvORyRAu4kE"
      },
      "outputs": [],
      "source": [
        "def sigmoid_function(z):\n",
        "  \"\"\"\n",
        "  Calcula la función sigmoide para z (de valor real)\n",
        "  \"\"\"\n",
        "\n",
        "  ### \n",
        "\n",
        "  probability = None\n",
        "\n",
        "  ###\n",
        "\n",
        "  return np.round(probability, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnAimtYOvVph"
      },
      "source": [
        "Esta función debería retornar los siguientes valores... \n",
        "\n",
        "- $\\Lambda(0) = 0.5$\n",
        "- $\\Lambda(-99) = 0$\n",
        "- $\\Lambda(2) = 0.8808$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvhiQ4t7v6_C"
      },
      "outputs": [],
      "source": [
        "assert sigmoid_function(0) == 0.5, f'La función arroja el valor de {sigmoid_function(0)} cuando debería ser 0.5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMlZbm4TwJQv"
      },
      "outputs": [],
      "source": [
        "assert sigmoid_function(-99) == 0, f'La función arroja el valor de {sigmoid_function(-99)} cuando debería ser 0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiuIOl76wJTL"
      },
      "outputs": [],
      "source": [
        "assert sigmoid_function(2) == 0.8808, f'La función arroja el valor de {sigmoid_function(2)} cuando debería ser 0.8808'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb1rLi3Umjl7"
      },
      "source": [
        "Adicionalmente, con ilustrativos, es necesario programar la función de verosimilitud (o binary crossentropy) para darle seguimiento a la evolución del modelo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEO6hGrpmwND"
      },
      "outputs": [],
      "source": [
        "def log_likelihood(y, X, beta):\n",
        "  \"\"\"\n",
        "  Retorna el valor de la función de Logaritmo de Verosimilitud para unos coeficientes estimados. \n",
        "  \"\"\"\n",
        "\n",
        "  ###\n",
        "  # 1. Calcular Predicciones dado el vector de coeficientes Λ(XB)\n",
        "  XB = None\n",
        "  LambdaXB = sigmoid_function(XB) # Dim: N x 1\n",
        "\n",
        "  # 2. Cálculo de Componentes de Función y*ln(Λ(XB)) + (1-y) * ln(1 - Λ(XB))\n",
        "\n",
        "  log_density = None \n",
        "\n",
        "  # Cálculo de la Suma\n",
        "  logLikelihood = log_density.sum()\n",
        "\n",
        "  ###\n",
        "\n",
        "  return logLikelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01mvu8GFoN29"
      },
      "source": [
        "# Paso 2. Actualización\n",
        "\n",
        "Construya una función que implemente la regla de actualización de los coeficientes. Si se busca maximizar la verosimilitud, entonces se necesita aplicar el Gradiente *Ascendente* como, \n",
        "\n",
        "$$\\beta_j^{(t+1)} = \\beta_j^{(t)} + \\alpha \\frac{\\delta \\ell(\\beta)}{\\delta \\beta_j } \\Big |_{\\beta^{(t)}},$$\n",
        "\n",
        "\n",
        "donde $\\alpha$ es la tasa de aprendizaje. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y-Xs1bGmf-r"
      },
      "outputs": [],
      "source": [
        "def update_vector_beta(beta_t, learning_rate, X, y):\n",
        "  \"\"\"\n",
        "  Implementa la regla de actualización del vector de coeficientes. \n",
        "\n",
        "  Parametros\n",
        "  ------------ \n",
        "    beta_t: \n",
        "      Vector de Coeficientes Actual. Dimensión (K x 1)\n",
        "\n",
        "    learning_rate:\n",
        "      Tasa de Aprendizaje\n",
        "\n",
        "    X:\n",
        "      Variables independientes. Dimensión (N x K)\n",
        "    \n",
        "    y:\n",
        "      Variable (binria) objetivo. Dimensión (N x 1)\n",
        "  \"\"\"\n",
        "\n",
        "  ###\n",
        "\n",
        "  # 1. Calcule las predicciones dado el vector de coeficientes Λ(XB)\n",
        "  # Pista: Calcule primero la combinación lineal y luego use la función sigmoide para volverlo probabilidades\n",
        "\n",
        "  XB = None\n",
        "  LambdaXB = None\n",
        "\n",
        "  # 2. Calcule la medición de \"error\" (y - Λ(XB))\n",
        "\n",
        "  error_vector = None \n",
        "\n",
        "  # 3. Utilice lo anterior para calcular el gradiente\n",
        "\n",
        "  gradient = None\n",
        "\n",
        "  # Implemente la regla de actualización\n",
        "\n",
        "  updated_beta = None\n",
        "\n",
        "  ###\n",
        "  return updated_beta # Debe ser vector de dimensión (K x 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGEPUpxrlZSA"
      },
      "source": [
        "# Paso 3. Inicialización del Algoritmo\n",
        "\n",
        "Una vez implementado el Gradiente Ascendente para maximizar la Verosimilitud, es necesario definir el valor inicial del vector de coeficientes, $\\beta$; la tasa de aprendizaje, $\\alpha$, y el número máximo de iteraciones a ejecutar. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMJ4Ynq-l3C3"
      },
      "outputs": [],
      "source": [
        "max_iter = 50000\n",
        "alpha = 0.01 # Pueden probar otros Learning Rates a ver qué les sale \n",
        "\n",
        "N, K = X.shape # X (N x k)\n",
        "\n",
        "beta_j = np.random.normal(size = (K, 1)) # (k x 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcbcZ6Od0zDc"
      },
      "outputs": [],
      "source": [
        "print(f'Vector de Coeficientes: {beta_j.T}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTn6Gi_BmKIJ"
      },
      "source": [
        "A continuación basta con generar el código que haga las iteraciones de la regla de actualización hasta alcanzar convergencia. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Urphdctp1TY5"
      },
      "outputs": [],
      "source": [
        "target_fn_value = []\n",
        "\n",
        "for idx in range(max_iter):\n",
        "\n",
        "  target_fn_value.append(log_likelihood(y, X, beta_j))\n",
        "\n",
        "  if idx > 0:\n",
        "    if np.linalg.norm(beta_prev - beta_j) < 1e-5:\n",
        "        break\n",
        "  # Guardado de Vector para seguimiento\n",
        "  beta_prev = beta_j\n",
        "\n",
        "  ### Implementación de la actualización (usar update_vector_beta)\n",
        "  beta_j = None\n",
        "\n",
        "  ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzkPTbyxpdz7"
      },
      "source": [
        "El vector de coeficientes estimados debería dar parecido a ```[[ 13.18208839  0.58965252 -5.1737186 ] ]```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuHx-al71m4Y"
      },
      "outputs": [],
      "source": [
        "print(beta_j.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9mrlDrvo1Id"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 5))\n",
        "plt.plot(target_fn_value)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('LogLikelihood')\n",
        "plt.title('Evolución de Log Verosimilitud')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrsUhPzLqX0x"
      },
      "source": [
        "# Paso 4. Visualización del Modelo\n",
        "\n",
        "Hasta este punto se encontró cuál es el vector $\\hat{\\beta}$ tal que $$\\hat{\\beta} = \\operatorname{arg}\\operatorname{min}_{\\beta} \\ell(\\beta).$$\n",
        "\n",
        "Con lo anterior se puede visualizar la frontera de predicción del modelo.\n",
        "\n",
        "Partiendo de que el modelo es, \n",
        "\n",
        "$$P(y_i = 1 | X; \\beta) = \\Lambda(\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i})$$\n",
        "\n",
        "Se puede mostrar que la frontera de decisión para $P(y_i = 1 | X; \\beta) = 0.5$, viene dada por, \n",
        "\n",
        "$$X_2 = \\frac{-\\beta_0 - \\beta_1 X_1}{\\beta_2}.$$\n",
        "\n",
        "\n",
        "A continuación programe la función que permita visualizar la frontera de predicción en los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8jzowTxqW6Q"
      },
      "outputs": [],
      "source": [
        "lin_space = np.linspace(-2, 5) # Corresponde a los puntos sobre el dominio de la función\n",
        "# Se calculará los puntos sobre la frontera en lin_space y luego se trazará la línea que los une. \n",
        "\n",
        "def frontera_prediccion(beta, lin_space):\n",
        "  \"\"\"\n",
        "  Permite calcular la ecuación de la recta que separa las dos clases (por probabilidad). \n",
        "\n",
        "  beta:\n",
        "    Vector de Coeficientes estimados (K x 1)\n",
        "  \"\"\"\n",
        "\n",
        "  ###\n",
        "\n",
        "  beta_0 = None\n",
        "  beta_1 = None\n",
        "  beta_2 = None\n",
        "\n",
        "  frontera = -(beta_0 - beta_1*lin_space)/(beta_2)\n",
        "\n",
        "  ###\n",
        "  return frontera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmikgCrNqXCq"
      },
      "outputs": [],
      "source": [
        "# Se generan nuevamente los datos\n",
        "X, y = make_blobs(n_samples=100, centers=2, n_features=2,\n",
        "                  random_state=0)\n",
        "plt.figure(figsize = (10, 5))\n",
        "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1])\n",
        "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1])\n",
        "plt.plot(lin_space, frontera_prediccion(beta_j, lin_space))\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVuWhBVKpvyv"
      },
      "source": [
        "# Comparación con Librería (Opcional)\n",
        "\n",
        "Para estar seguros que la implementación quedó bien hecha, se recomienda comparar con el modelo implementado en alguna librería. Para este caso, se sugiere el uso del estimador [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzQbYQ4m65Fk"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yIv__AXs-Ij"
      },
      "source": [
        "El estimador de Regresión Logística de Sci-Kit Learn incluye por defecto la regularización de los coeficientes. Por lo tanto, si no se \"desactiva\" no van a tener los mismos resultados. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi1EcN9C1tIR"
      },
      "outputs": [],
      "source": [
        "### Use el Estimador LogisticRegression para ajustar los datos \n",
        "clf = LogisticRegression(random_state=0, penalty = 'none').fit(X, y)\n",
        "\n",
        "###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELE5EI7QtNJv"
      },
      "outputs": [],
      "source": [
        "beta_coef_lr = np.hstack([clf.intercept_.reshape(-1, 1), clf.coef_]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT-8MLNG2yUw"
      },
      "outputs": [],
      "source": [
        "print(beta_coef_lr.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Emzb0FO8byv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 5))\n",
        "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1])\n",
        "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1])\n",
        "plt.plot(lin_space, frontera_prediccion(beta_coef_lr, lin_space), color = 'red', label = 'Regresión Logística (Sci-Kit Learn)')\n",
        "plt.plot(lin_space, frontera_prediccion(beta_j, lin_space), color = 'purple', label = 'Gradient Descent')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfD1MoK_n2zK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
